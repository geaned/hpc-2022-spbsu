# Лекция 9

## Kernel-space RCU

Пусть мы хотим написать структуру данных в ядре ОС, которая не будет давать **никакой overhead для читателей**; фактически, структура ориентирована, в первую очередь, на чтение из нее. Читатели независимо работают со структурой, и для удобства будем считать, что чтение из структуры происходит в рамках одного системного вызова, обрамленного функциями типа `rcu_enter()` и `rcu_leave()` (на данный момент, с пустыми реализациями из-за текущего условия).

Но есть и писатель, который **не должен конфликтовать с читателями**, а именно ждать завершения работы всех текущих читателей, чтобы, к примеру, не начать удалять вершину, из которой некоторый поток все еще читает (иначе есть риск segmentation fault).

*Вообще сложность реализации, в первую очередь, связана именно с операцией удаления, поскольку при добавлении элемента все просто: до некоторого момента все потоки будут идти по одному пути, а после него --- по другому.*

Более формально, когда писатель вызывает `remove()`, происходит перемещение ссылки в структуре (операция remove), потом он с помощью `rcu_sync()` начинает некоторый grace period, в котором ждет, пока текущие читатели не закончат работу, после которого осуществляет освобождение памяти (операция reclamation).

Считаем, что ядро ОС собрано без флага `CONFIG_PREEMPT` --- это означает, что если ОС выдала потоку квант времени планирования (т.е. квант времени исполнения на ядре процессора), то поток не может быть снят с исполнения во время системного вызова.

Идея ждать, пока системных вызов к нашей структуре в некоторый момент времени не будет или пока страницы с процессами, которые обращаются в нашей структуре, не будет размаплены не годится, поскольку она заставить нас ждать не момента, в который со структурой перестали работать все текущие читатели, а момента, в который со структурой перестали работать читатели вообще, а таковой может и не наступить.

На самом деле отсутствие флага `CONFIG_PREEMPT` --- это большая подсказка, ведь это означает, что если на ядре в момент времени работает читатель (через системный вызов), то писатель на нем **точно не будет работать**, пока текущее чтение не закончится.

**Ответ:** `rcu_sync()` должен запускать цикл по ядрам процессора, на каждом из которых нужно попытаться сделать хотя бы "ничто" (т.е. просто встать на исполнение).

Однако, данный алгоритм не предусматривает сборку ядра с флагом `CONFIG_HOTPLUG_CPU`, позволяющий менять процессоры в системе во время работы ОС, посольку текущие потоки-читатели при удалении процессора из системы могут быть переброшены на уже обработанные в `rcu_sync()` ядра.

Вообще, весь класс алгоритмов, одна из которых только что был рассмотрен, называется **epoch-based synchronization**. Реально для того, чтобы обработать ситуацию с тем, что поток могут снять с исполнения `syscall`, можно добавить счетчик, отображающий глобальную эпоху структуры данных, который меняет только писатель. В такой постановке читатель ждет, пока в TLS каждого писателя будет записан достаточно большой номер эпохи.

## User-space RCU

У каждой структуры добавляются двое членов-данных: **глобальный счетчик эпохи** `globalCtl` и мьютекс для читателей `rcu_mutex`. У каждого потока есть указатель на следующий TLS `next` и **собственное значение эпохи** `ctl`.

Прибилзительный код читателя:

```c++
void rcu_enter() {
	rcu_thread_data* rec = get_thread_record();
	uint32_t tmp = rec->ctl.load();
	// ...
	rec->ctl.store(globalCtl.load());
	// ...
}

void rcu_leave() {
	recu_thread_data *rec = get_thread_record();
	rec->ctl.increment();
}
```

Прибилзительный код писателя:

```c++
void rcu_sync() {
	std::unique_lock<std::mutex> sl(rcu_mutex);
	inc_and_wait();
}

void inc_and_wait() {
	globalCtl.increment();
	foreach (rcu_thread_data* rec) {
		while (rec->ctl.load() != globalCtl) {
			yield();	// ожидание на back-off стратегии
		}
	}
	// ...
}
```

*В общем случае, если мы пишем на Java, RCU-техники особо не нужны из-за поведения сборщика мусора. Разве что если нужно конкретно реализовать логику того, что после удаления элемента из структуры, потоки-читатели гарантированно не будут с ним работать, то придется делать это самостоятельно.*

## Flat-combining

Пусть у нас есть потокоопасная структура данных. На пятой лекции мы уже познакомились с тем, какими способами можно добавить потокобезопасность в структуру, но бывает так, что она очень сложная, а грубой синхронизации недостаточно по производительности.

Используем отдельный класс `FlatCombiner`, который агрегирует структуру и предоставляет интерфейс типа `do_operation(Type type, vararg args)`. Внутри `FlatCombiner` есть примитив синхронизации `m` и быстрая очередь из TLS (не принципиально использовать TLS, скорее нужно для скорости), в которую попадают задания на работу со структурой.

Как задания попадают в очередь? Рассмотрим операцию `register(int tid)`, который зарегистрирует новый поток и добавит соответствующий ему TLS в очередь.

Приблизительный код `do_operation`:
```c++
do_operation() {
	// публикация операции в TLS, который
	// соответствует потоку, вызвавшему функцию
	if (m.try_lock()) {
		// наш поток --- поток-комбайнер,
		// выполняем в нем все операции
	} else {
		yield();
	}
}
```

Реально со структурой одновременно работает только один поток. Преимущества:

- увеличивается переиспользование кэша ядра, поскольку один поток подряд выполняет все операции в очереди
- есть свобода для оптимизации некоторых операций со структурой (например, объединять вставки, если это поможет нам меньше суммарно бегать по структуре и искать, куда вставлять элементы)
- сама структура данных не меняется
- небольшое ускорение за счет использования TLS и уменьшения количества переключений контекста потока

*Почему бы не заставить отдельный поток решать все поступающие задачи? Небольшой минус: при отсутствии задач, поток будет простаивать и при очередном поступлении задач, нужно тратить время на его пробуждение.*

Подобные техники используются в промышленности, но не всегда: **может быть и деградация системы**. Также у этого решения много степеней свободы (стратегия backoff, структура очереди, политика работы с мьютексом и т.д.), поэтому нет фреймворка, который позволит применить это решение к абсолютно любой структуре данных.

## Архитектура thread pool в Java

Пулы потоков в Java:
- FixedThreadPool
- SingleThreadPool
- CachedThreadPool (меняет количество потоков в зависимости от нагрузки)
- ScheduledThreadPool (выполняет задачи по периодическому таймеру)
и многие другие (например, ForkJoinPool)

Они все наследуются от `ExectuorService`. Его базовая реализация состоит из структуры данных, хранящей потоки `Thread` (изначально потоки находятся на `wait()`), и очередь задач, в которую с помощью `sumbit()` можно передать объект, реализующий интерфейс `Callable<T>` с методом `T call()`.

Внутри `sumbit()` мы добавляем объект в очередь, будим **хотя бы один поток** с помощью `notify()` и создаем и возвращает объект типа `Future<T>` --- контейнер с блокирующим методом `T get()`, который возвращает результат, когда он будет готов (или бросает `ExecutionException`, из которого можно узнать настоящую причину с помощью метода `cause()`).

*Еще у `Future<T>` есть метод `cancel()`, позволяющий отменить выполнение задачи (и задача будет убрана из очереди).*

Преимущества использования `ThreadPool`:
- переиспользование потоков
- разделение уровней абстракции задачи и потока
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTc2NDkyNzQ2XX0=
-->