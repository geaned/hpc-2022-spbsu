# Лекция 9

## Kernel-space RCU

Пусть мы хотим написать структуру данных в ядре ОС, которая не будет давать **никакой overhead для читателей**; фактически структура ориентирована в первую очередь на чтение из нее. Читатели независимо работают со структурой, и для удобства будем считать, что чтение из структуры происходит в рамках одного системного вызова, обрамленного функциями типа `rcu_enter()` и `rcu_leave()` (на данный момент, с пустыми реализациями из-за текущего условия).

Но есть и писатель, который **не должен конфликтовать с читателями**, а именно ждать завершения работы всех текущих читателей, чтобы, к примеру, не начать удалять вершину, из которой некоторый поток все еще читает (иначе есть риск segmentation fault).

*Вообще сложность реализации, в первую очередь, связана именно с операцией удаления, поскольку при добавлении элемента все просто: до некоторого момента все потоки будут идти по одному пути, а после него --- по другому.*

Более формально, когда писатель вызывает `remove()`, происходит перемещение ссылки в структуре (операция "removal"), потом он с помощью `rcu_sync()` начинает некоторый grace period, в котором ждет, пока текущие читатели не закончат работу, после которого осуществляет освобождение памяти (операция "reclamation").

Считаем, что ядро ОС собрано без флага `CONFIG_PREEMPT` --- это означает, что если ОС выдала потоку квант времени планирования (т.е. квант времени исполнения на ядре процессора), то поток не может быть снят с исполнения во время системного вызова.

Идея ждать, пока системных вызов к нашей структуре в некоторый момент времени не будет или пока страницы с процессами, которые обращаются в нашей структуре, не будут размаплены, не годится, поскольку она заставит нас ждать не момента, в который со структурой перестали работать все текущие читатели, а момента, в который со структурой перестали работать читатели вообще, а таковой может и не наступить.

На самом деле отсутствие флага `CONFIG_PREEMPT` --- это большая подсказка, ведь это означает, что если на ядре в момент времени работает читатель (через системный вызов), то писатель на нем **точно не будет работать**, пока текущее чтение не закончится.

**Ответ:** `rcu_sync()` должен запускать цикл по ядрам процессора, на каждом из которых нужно попытаться сделать хотя бы "ничто" (т.е. просто встать на исполнение).

Однако, данный алгоритм не предусматривает сборку ядра с флагом `CONFIG_HOTPLUG_CPU`, позволяющий менять процессоры в системе во время работы ОС, поскольку текущие потоки-читатели при удалении процессора из системы могут быть переброшены на уже обработанные в `rcu_sync()` ядра.

Вообще, весь класс алгоритмов, один из которых только что был рассмотрен, называется **epoch-based synchronization**. Реально для того, чтобы обработать ситуацию с тем, что поток могут снять с исполнения `syscall`, можно добавить счетчик, отображающий глобальную эпоху структуры данных, который меняет только писатель. В такой постановке читатель ждет, пока в TLS каждого писателя будет записан номер текущей эпохи.

## User-space RCU

У каждой структуры добавляются двое членов-данных: **глобальный счетчик эпохи** `globalCtl` и мьютекс для читателей `rcu_mutex`. У каждого потока есть указатель на следующий TLS `next` и **собственное значение эпохи** `ctl`.

Прибилзительный код читателя:

```c++
void rcu_enter() {
	rcu_thread_data* rec = get_thread_record();
	uint32_t tmp = rec->ctl.load();
	// ...
	rec->ctl.store(globalCtl.load());
	// ...
}

void rcu_leave() {
	rcu_thread_data *rec = get_thread_record();
	rec->ctl.increment();
}
```

Прибилзительный код писателя:

```c++
void rcu_sync() {
	std::unique_lock<std::mutex> sl(rcu_mutex);
	inc_and_wait();
}

void inc_and_wait() {
	globalCtl.increment();
	foreach (rcu_thread_data* rec) {
		while (rec->ctl.load() != globalCtl) {
			yield();	// ожидание на back-off стратегии
		}
	}
	// ...
}
```

*В общем случае, если мы пишем на Java, RCU-техники особо не нужны из-за поведения сборщика мусора. Разве что если нужно конкретно реализовать логику того, что после удаления элемента из структуры, потоки-читатели гарантированно не будут с ним работать, то придется делать это самостоятельно.*

## [Flat-combining](https://storage.yandexcloud.net/lms-vault/private/1/courses/2019-spring/spb-hp-course/materials/flat-combining.pdf?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=YCAJEG-LFlOUp7t_VtjANSWBT%2F20230117%2Fru-central1-a%2Fs3%2Faws4_request&X-Amz-Date=20230117T185556Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=7c7782d4caea5d32c157ba9fe22944ccccc515beb980a2ba78b07fe163b80f32)

Пусть у нас есть потокоопасная структура данных. На пятой лекции мы уже познакомились с тем, какими способами можно добавить потокобезопасность в структуру, но бывает так, что она очень сложная, а грубой синхронизации недостаточно по производительности.

Используем отдельный класс `FlatCombiner`, который агрегирует структуру и предоставляет интерфейс типа `do_operation(Type type, vararg args)`. Внутри `FlatCombiner` есть примитив синхронизации `m` и быстрая очередь из TLS (не принципиально использовать TLS, они скорее нужны для скорости), в которую попадают задания на работу со структурой.

Как задания попадают в очередь? Рассмотрим операцию `register(int tid)`, который зарегистрирует новый поток и добавит соответствующий ему TLS в очередь.

Приблизительный код `do_operation`:
```c++
do_operation() {
	// публикация операции в TLS, который
	// соответствует потоку, вызвавшему функцию
	if (m.try_lock()) {
		// наш поток --- поток-комбайнер,
		// выполняем в нем все операции
	} else {
		yield();
	}
}
```

Реально со структурой одновременно работает только один поток. Преимущества:

- увеличивается переиспользование кэша ядра, поскольку один поток подряд выполняет все операции в очереди
- есть свобода для оптимизации некоторых операций со структурой (например, объединять вставки, если это поможет нам меньше суммарно бегать по структуре и искать, куда вставлять элементы)
- сама структура данных не меняется
- небольшое ускорение за счет использования TLS и уменьшения количества переключений контекста потока

*Почему бы не заставить отдельный поток решать все поступающие задачи? Небольшой минус: при отсутствии задач поток будет простаивать, и при очередном поступлении задач нужно тратить время на его пробуждение.*

Подобные техники используются в промышленности, но не всегда: **может быть и деградация системы**. Также у этого решения много степеней свободы (стратегия backoff, структура очереди, политика работы с мьютексом и т.д.), поэтому нет фреймворка, который позволит применить это решение наиболее производительно к абсолютно любой структуре данных.

## Архитектура thread pool в Java

Пулы потоков в Java:
- FixedThreadPool
- SingleThreadPool
- CachedThreadPool (меняет количество потоков в зависимости от нагрузки)
- ScheduledThreadPool (выполняет задачи по периодическому таймеру)
- и многие другие (например, ForkJoinPool)

Они все наследуются от `ExectuorService`. Его базовая реализация состоит из структуры данных, хранящей потоки `Thread` (изначально потоки находятся на `wait()`), и очереди задач, в которую с помощью `submit()` можно передать объект, реализующий интерфейс `Callable<T>` с методом `T call()`.

Внутри `submit()` мы добавляем объект в очередь, будим **хотя бы один поток** с помощью `notify()`, создаем и возвращаем объект типа `Future<T>` --- контейнер с блокирующим методом `T get()`, который возвращает результат, когда он будет готов (или бросает `ExecutionException`, из которого можно узнать настоящую причину с помощью метода `cause()`).

*Еще у `Future<T>` есть метод `cancel()`, позволяющий отменить выполнение задачи (и задача будет убрана из очереди).*

Преимущества использования `ThreadPool`:
- переиспользование потоков
- разделение уровней абстракции задачи и потока
