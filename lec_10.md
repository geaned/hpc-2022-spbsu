# Лекция 10

## Профилировка приложений

**Фабула**: заказачик прохдит с приложением и говорит, что он недоволен его производительностью. Как разработчики, мы должны выполнить следующую **задачу**: повысить производтельность доступными методами или же сказать, что это сделать невозможно.

Запускаем профилировщик `valgrind --tool=callgrind` на нашем приложении, получаем файл со всеми вызовами, для просмотра которого используем `kcachegrind`.

*`kcachegrind` использует `graphviz` для визуализации графа.*

Дальше начинаем процесс оптимизации: читаем API используемых библиотек, пробуем использовать более быструю память, переписываем алгоритмическую часть, но, допустим, каждое улучшение ускоряет приложение на 1 микросекунду.

На самом деле, **профилировщик не показывает настоящее время работы программы**, а опирается на процессорное время; перед профилировкой есть смысл, к примеру, воспользоваться простой утилитой `time`, которая в `real` показывает астрономическое время исполнения программы, а в `user` и `sys` указано процессорное время. Следовательно, условный `sleep` не отловится профилировщиком, но `time` с легкостью на него укажет.

**Вывод**: пользуясь инструментом анализа, нужно понимать какой анализ он предоставляет.

Рассмотрим профилировщик **VTune**, который предоставляет гораздо более подробный анализ. Спрофилируем приложение по hotspots и увидим приблизительно то же самое, что и в valgrind (разве что здесь сразу видны астромномическое и процессорное время и есть отдельный график загруженности ядра потоком). Однако профилировка по threading сразу покажет объекты, на которых было самое долгое ожидание (а `sleep` попадает под это определение, поскольку реализован через условную переменную).

Профилировщики вроде VTune работают совсем не так, как valgrind. Архитектурно это сепмлирующие профилировщики; они делают множество снимков и сохраняются вместе со своими трассами. Делается это на основе механизма, аналогичного механизму LD preload.

## LD preload

Пусть есть следующие файлы с кодом:

```c++
a.h
void doA();

b.h
void doB();

a.cpp
void doA() {
	doX();
}
void doX() {
	std::cout << "a";
}

b.cpp
void doB() {
	doX();
}
void doX() {
	std::cout << "b";
}
```

И мы из этих файлов делаем две разделяемые библиотеки: `a.so` и `b.so`. Другой разрабочик пишет `main.cpp` с кодом:

```c++
int main() {
	doB();
	doA();
	return 0;
}
```

Этот файл собирается с помощью `g++ main.cpp -la -lb -o a.out`. Далее при исполнении `./a.out` **выводом будет `aa`.**

Почему? Когда мы собираем shared object файлы, они так же, как и `a.out`, являются **elf-файлами (executable linkable format)**. В них есть заголовки с разделяемыми символами: всеми именами функций и переменных, которые должны быть видны извне. Символы бывают разрешенными и неразрешенными: первые реализованы в файле, в котором используются, а другие --- нет.

Загрузчик исполняемых файлов вначале формирует таблицу разделяемых символов со всем множеством используемых символов, в которую он записывает адреса, по которым нужно обращаться коду, которому понадобился тот или иной символ (таблица формируется отдельно для каждого процесса).

Библиотеки линукются к исполняемому файлу **в порядке тополоической сортировки** (отношение в графе --- зависимость одной библиотеки от другой), а библиотеки на одном уровне в дереве зависимости, имеющие общего предка, подгружаются в порядке линковки (в исполняемой при сборке команде). При этом для каждого символа загрузчик интересует только **первое вхождение функции в разделяемую библиотеку**. По этой причине, в нашем случае, в таблице символов символу `doA`  соответствовала функция из файла `a.so`.

Как решить эту проблему? Два способа:
- упаковать в анонимный namespace (будет использоваться namespace с названием, выраженным случайной строкой)
- объявить функцию как `static` (функция не попадет в список разделяемых символов)

## Профилировка приложений: продолжение

Как же LD preload относится к работе профилировщиков? В ОС есть возможность подгрузить бибилиотеку до подгрузки всех остальных бибилиотек вне зависимости от того, линкуется ли с ней приложение изначально.

Так, например, можно подменить код функции на свою реализацию, например, вместо `send(void* data)` из библиотеки `somelib`, который просто отправляет данные, подключить свою функцию **с такой же сигнатурой**, которая отправит данные по какому-нибудь адресу и уже потом вызовет `somelib::send(data)`. На Linux это делается с помощью `export LD_PRELOAD=<PATH_TO_SO_FILE>`.

Аналогично делает и профилировщик, фактически подменяя вызов `main()` нашего файла на вызов `main()` через свою функцию. Безусловно, при таком запуске приложение будет работать дольше. Насколько? Тут уже по ситуации, например, `valgrind` дает **возрастание времени исполнения до 120 раз**.

Рассмотрим еще один момент, влияющий на производительность. Рассмотрим функции на языке C++:

```c++
int sumMatrixRows(const MATRIX_TYPE& matrix) {
	int sum = 0;
	for (int i = 0; i < MATRIX_SIZE; i++)
		for (int j = 0; j < MATRIX_SIZE; j++)
			sum += matrix[i][j];
	return sum;
}

int sumMatrixCols(const MATRIX_TYPE& matrix) {
	int sum = 0;
	for (int j = 0; j < MATRIX_SIZE; j++)
		for (int i = 0; i < MATRIX_SIZE; i++)
			sum += matrix[i][j];
	return sum;
}
```

Какая функция отработает быстрее и какая будет причина? Первая функция отработает почти в два раза быстрее из-за того, что при выполнении второй функции будет на порядок больше **промахов по кэшу процессора**, что показывает профилировка по memory access. Разница в реализациях функции играет роль даже при компиляции с оптимизациями: первая функция снова в два раза быстрее.

В профилировщиках есть еще одна интересная метрика, на которую стоит смотреть. Она называется **CPI Rate** и показывает потенциальную возможность оптимизировать приложение с технической точки зрения. Конкретно она говорит, сколько инструкций в среднем выполняется за один процессорный такт. Теоретически идеальное значение метрики равно **0.25**.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTU5MzY4OTIwMV19
-->