# Лекция 6

## Поиск ошибки в многопоточном приложении

**Фабула**: есть система, состоящая из
- двух камер на поворотном устройстве
- сервера на Linux с многопоточным приложением, на который передаются данные с камер
- удаленного рабочего стола, с которого на сервер по сети через десктопное приложение пользователь может смотреть видео, управлять положением камер и т.п.

Потоки в приложении условно делятся по следующим функциям:
- один поток принимает даные с камер
- пара потоков занимается видеоаналитикой
- один поток (для простоты) занимается связью с удаленным рабочим столом

**Проблема**: приблизительно раз в сутки пользователь на удаленном рабочем столе видит, что изображение зависает на некоторый промежуток времени.

**Задача**: провести необходимый анализ, найти проблему и по возможности решить ее.

> Система не может позволить себе замереть ни на секунду. С момента входа объекта в поле зрения камер до момента выхода проходит, как правило, четверть секунды. За это время нужно принять видео, обработать, провести видеоаналитику, выдать и сопроводить управляющее воздействие.

*На сколько зависает?*

> Если ничего не предпринимать, то фактически навсегда.

*Есть ли дайджест по потреблению оперативной памяти, ресурсов центрального процессора и т.п. в момент зависания?*

> Есть, ничего нештатного не обнаружено.

*Что мы предпринимаем на данный момент, чтобы система продолжила работу?*

> Перезапуск приложения на сервере помогает.

*Можно ли посмотреть логи входящих на сервер запросов в момент, когда случилось зависание?*

> После зависания логи перестают писаться.

*Перестает ли работать интерфейс на удаленном рабочем столе в момент зависания?*

> Нет, управление не зависает, но отображаемое состояние не изменяется.

*Как насчет воспользоваться сниффером пакетов, например, Wireshark, чтобы понять, приходят ли пакеты с сервера в ответ на исходящие запросы?*

> Посмотрели: запросы уходят, но ответы перестают приходить. Как результат, через некоторое время и запросы перестанут уходить, поскольку перестали приходить acknowledgements. Аналогично, посмотрели для камер: они продолжают слать видео.

Через `netstat -nlp` можно посмотреть открытые на машине порты, наличие соединений через них и их статус.

*Зависит ли воспроизводимость ошибки от данных, принимаемых с камер?*

> Нет, даже если поддельными запросами отправлять черный кадр, ошибка воспроизводится.

*Воспроизводится ли ошибка в одно и то же время?*

> Нет, в разное.

*Посмотрим на сервере, сколько процессорного времени выделяется потокам процесса приложения.*

> Из 8 ядер на сервере постоянно 2-3 ядра свободны. Когда происходит зависание, ресурсов потребляется немного меньше.

*Посмотрим с помощью `valgrind`, есть ли потенциальные ошибки параллельного программирования в коде решения.*

> Потратили где-то человеко-неделю, ошибок не найдено.

*Есть ли детерминированная запись, на которой заканчивается каждый лог?*

> Нет, всегда на разной.

*Подключимся к процессу при зависании с помощью `gdb` и посмотрим, где в момент зависания назодятся потоки.*

> Сделать это можем, но есть нюанс: как только мы это делаем, зависание прекращается.

*Пересоберем приложение другим компилятором, с оптимизациями и без.*

> Ошибка не исчезает.

*Зажурналируем приоритеты потоков в процессе.*

> Никаких аномалий.

*Проверим покрытие кода тестами и скажем разработчикам пересмотреть код.*

> Добавили тестов, ошибок не найдено.

*Снимем дамп памяти процесса при зависании, посмотрим, где находятся потоки.*

> Сняли, не нашли ничего необычного

Дамп процесса в произвольный момент времени можно снять: искусственно вызовем segmentation fault в приложении с помощью отправки сигнала и в ответ будет сгенерирован core dump.

*Воспроизводится ошибка на таком же другом железе?*

> Воспроизводится.

*А если взять процессор с меньшим количеством ядер?*

> На двух или четырех ядрах работает идеально и не зависает.

*Может ли окружение, в котором работает приложение, повлиять на его работу?*

> Посмотрели глазами: ничего не нашли.

Ожидалось, что будет упомянуты `strace -p <PID>` для просмотра системных вызовов, которые совершает наше приложение. Отсюда можно увидеть объективную картинку взаимодействия приложения с ОС со всеми агрументами. В нашем случае мы подключаемся с его помощью к процессу, процесс пробуждается и мы наблюдаем первые системные вызовы с момента пробсуждения. В них встречается `FUTEX`, но просто так на столь базовый объект ОС Linux грешить не стоит.

Были вопросы касательно того, можем ли мы узнать, работал или не работал поток в последнее время, или увидеть текущий стек вызовов. На самом деле это можно сделать с помощью **виртуальной файловой системы (VFS)**, в которую входят каталоги `/dev`, `/proc` и `/sys`.

Ищем файл `/proc/<PID>/task`, каталоги в котором ссответствуют вычислителным потокам интересующего нас процесса. Нам нужно узнать, какие потоки зависли и на чем. Открываем  файл `/proc/<PID>/task/<TID>/sched`; в нем есть величина `nr_switches` --- количество раз, которые этот поток вставал на исполнениена каком-либо ядре процессора.

Применительно к нашей задаче, отсюда мы можем узнать, какие потоки подвисли. Для них можно открыть `/proc/<PID>/task/<TID>/stack`.

Еще с помощью `strace` можно получить статистику по системным вызовам с помощью `strace -c -p <PID>`, в полученной таблице есть графа `errors` --- количество ошибок на системном вызове.

В нашей задаче оказалось, что подозрительно много ошибок случалось на `futex`. Это оказалось **проблемой ядра ОС Linux**. На ее [исправление](https://github.com/torvalds/linux/commit/76835b0ebf8a7fe85beb03c75121419a7dec52f0) ушло около года из-за нетривиальных симптомов.

## Приближенное устройство современного процессора

Рассмотирим два ядра одного процессора и их кэши первого уровня (вообще уровней кэша в современных процессорах больше, но сейчас это непринципиально). Данные в кэш всегда попадают из оперативной памяти. **Кэш** --- это ассоциативная память, самая быстрая после регистровой памяти, **состоящая из линий (cache lines)**. Размер кэша варируется, но на большинстве современных процессоров он равен сотням килобайт (через `lscpu`), а размер линейки кэша равен 64 байтам (через `cat /proc/cpuinfo`).

Процессор подгружает данные с ОЗУ именно линейками и не берет отдельно куски данных произвольного размера. Допустим, мы хотим сделать инкремент числа. Считывание его из оперативный памяти реализовано с помощью специального протокола, завязанного на общении с некоторой сущностью, которую мы будем называть **брокером** (один на весь процессор).

Ядро запрашивает чтение данных у брокера: если данных в кэше никакого ядра, линейка с данными копируется и делается инкремент. Пусть другое ядро тоже захотело поработать с этой переменной; в таком случае повторного чтения не будет и будет произведен перенос линейки в кэш этого ядра.

Для поддержки когерентности данных между кэшами процессора существует **протокол поддержки когерентностей кэшей процессора (MESI)**, по которому у каждой линейки каждого кэша стоит один из четырех флагов:

- **I (invalid)**: флаг по умолчанию --- данных нет
- **E (exclusive)**: данные запрошены одним ядром (не синхронизируются с оперативной памятью)
- **M (modified)**: данные запрошены одним ядром и изменены в линейке (синхронизируются с оперативной памятью)
- **S (shared)**: данные запрошены несколькими ядрами

В состояние S можно попасть как из состояния E, так и из состояния M.

Теперь у нас в обоих линейках записано инкрементированное число и стоит флаг S, в оперативной памяти --- все еще страое значение. Когда новое ядро снова инкрементирует значение, линейка уже будет помечена как shared и брокеру отправится **пара сообщений read-invalidate**: остальные ядра должны инвалидировать линейку у себя и перечитать ее из кэша ядра, в котором был сделан инкремент. Само ядро **ждет получение invalidation acknowledgements** через брокера от всех ядер, с которыми линейка с рассматриваемой переменной у него является общей.

Теперь рассмотрим вариант, оптимизированный для того, чтобы ядра друг друга не дожидались. У каждого ядра добавляются аппаратные стрктуры **store buffer и invalidate queue**. Возвращаемся к моменту перед повторным инкрементом значения в shared линейке. По порядку произойдет следующее:

- ядро сохранит новую линейку кэша в store buffer
- ядро пошлет пару запросов read-invalidate и **продолжает работу**
- read-invalidate придет в invalidate queue ядер, разделяющих рассматриваемую линейку кэша (кроме той, в которой произошел инкремент); у ядер нет обязанности сразу инвалидировать линейку, они продолжат работу и проведут инвалидацию, когда им удобно
- ядра отправят invalidation acknowledgements, **как только получают запрос**
- при получении всех необходимых invalidation acknowledgements ядро загрузит ранее сохраненную линейку из store buffer и удалит запись

*Store buffer нужен на случай, когда ядро решило применить еще одну модификацию к неподтвержденной shared линейке кеша. Если оно увидит, что store buffer не пуст, ему придется дождаться всех invalidation acknowledgements. Если линейка лежит в store buffer, при последующих операциях, не подифицирующих ее, будет использоваться ее версия именно из store buffer.*

Когда подобная реализация может породить проблему? Рассмотрим два потока, параллельно выполняющие, соответственно, две функции:
```c++
void f() {	// поток [1]
	a = 1;
	b = 1;
}
void g() {	// поток [2]
	while (b == 0)
		continue;
	assert(a == 1);
}
```

В первом потоке есть shared переменная `a` и exclusive переменная `b` (обе равны нулю ), во втором --- только  shared переменная `a`. Возможен следующий план выполнения:

```
[1] a = 1; отправляет a в store buffer, отправляет запрос read-invalidate (**) для a
[2] while (b == 0); отправляет запрос read (*) для b 
[1] b = 1; сделает b modified
[1] получает запрос read (*) for b; делает b shared
[2] получает b по запросу (*)
[2] b = 1; => выходит из цикла
[2] assert(a == 1); бросает AssertionError
[2] отправляет запрос read-invalidate (**) для a, отправляет invalidation acknowledgement (**)
[1] получает invalidation acknowledgement (**), сбрасывает store-buffer для a
```

А запрос на read-invalidate (**) пришел бы только после того, как мы получили `AssertionError`.

В первой же модели такой проблемы бы не возникло, поскольку предпоследняя строка (вместе с получением нового значения `a`) должна была бы выполниться между второй и третьей. Именно для таких случаев и нужны барьеры памяти:

- `smp_rmb()` применяет все инструкции из invalidate queue
- `smp_wmb()` ждет все invalidation acknowledgements (и берет запись из store buffer)
- `smp_mb()` выполняет `smp_rmb()` и `smp_wmb()` с оптимизацией

## Коротко о volatile

Ключевое слово **volatile** на разных ЯП означает разные вещи. На C++ оно было добавлено до многопоточности и не защищает в полной мере от ошибок параллельного программирования: оно говорит, что каждый раз при чтении переменной ее нужно честно считать из оперативной памяти; в Java же оно явно связано с применением необходимых барьеров памяти.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4MDMxNTQxMzldfQ==
-->