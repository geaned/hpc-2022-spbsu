# Лекция 13

## Транзакционная память

Вообще в параллельном программировании разработчики часто сталкиваются с нетривиальными проблемами, приводящими к дедлокам, гонкам данных и тому подобному, или с необходимостью реализовать тот или иной lock-free или wait-free алгоритм, поэтому появилось желание уметь это делать **безболезненно и достаточно производительно**.

*Более точечные примеры проблем:*
- *отсутствие связи ресурс-примитив (фактически, она есть только в уме разработчика)*
- *сложный контроль времени жизни объектов (актуально для языков без реализованного сборщика мусора)*
- *инверсия приоритетов*
- *сложность алгоритмов, работающих на CAS*
- *наличие необхоимости в атомарности операций над несколькими структурами данных*

Это привело к появлению **транзакционной памяти**.

Вообще когда приложение работает с базой данных под управлением СУБД, это делается через **менеджер транзакций**, внутри которого зашита очень сложная логика, направленная на производительную работу, к примеру, перестановка конфликтующих операций таким образом, чтобы достичь минимально возможного количества конфликтов.

Почему бы не сделать что-то аналогичное для связи кода, написанного разработчиком, с оперативной памятью? Например, чтобы для реализации метода вставки в lock-free очередь достаточно было бы написать что-нибудь вроде:

```java
public void enqueue(T item) {
	atomic {
		Node node = new Node(item);
		node.next = tail;
		tail = node;
	}
}
```

Концептуально это можно реализовать на уровне языка программирования (software transactional memory --- с ним можно столкнуться, например, в языке Haskell), однако фактически lock-free алогритм, реализованный с помощью подобного механизма, **на самом деле не будет lock-free**, поскольку использование примитивов синхронизации уже будет вшито в сам механизм.

Преимущества:

- отсутствие явных блокировок в коде
- возможность контроля транзакций (к примеру, поведения при отсутствии успеха)
- лучшая утилизация ресурсов за счет того, что дополнительные механизмы, представляемые транзакционной памятью, отрабатывают только в случае, если что-то пошло не так
- повышение уровня абстракции

Или же транзакционную память можно реализовать на уровне архитектуры процессора (hardware transactional memory), а точнее на уровне линеек кэшей. Для этого помечаем все линейки кэша, используемые во время транзакции, с помощью отдельного флага, и если другой поток в транзакции будет работать с этими данными, то мы сделаем в нем rollback. А как поддержать связь между потоками, параллельно исполняющими транзакции? Для этого у нас **уже есть протокол поддержки когерентности кэшей процессора**, достаточно добавить необходимые ассемблерные инструкции (и реализовать биты транзакционности).

Первое ограничение --- все, с чем мы работаем внутри транзакций, **должно помещаться в кэш**. Второе ограничение уже временное --- **поток должен уложиться в свой квант времени планирования** (поскольку кэш не сохраняется при смене потока на ядре).

*Несмотря на ограничения, транзакционная память часто применяется в структурах данных, и для использования ее в коде существуют шаблоны программирования, например, **lock teleportation**. Он заключается в том, чтобы объединять несколько итераций внутри одного прохода по структуре данных с тонкой синхронизации в транзакцию.*

К сожалению, архитектуры современных процессоров настолько сложны, что текущие попытки реализации транзакционной памяти на уровне процессора приводят к багам, которые очень опасны для боевых систем.

## MPI

**Message Passing Interface (MPI)** --- клоссплатформенный программный интерфейс, который позволяет процессам, выполняющим одну задачу, обмениваться сообщениями между собой.

*Здесь и далее мы будем употреблять слово **кластер**. Для нас кластер --- это, фактически, набор машин, взаимодействующих друг с другом и представленных нам в качестве единого вычислительного ресурса.*

Сборка MPI-приложения осуществляется с помощью отдельной утилиты, для C ей будет `mpicc` (пример команды: `mpicc -o mpihello mpihello.c`), а запуск производится через `mpirun`: `mpirun -np <CLUSTER_SIZE> mpihello`. На всех машинах запускается один и тот же исполняемый файл, а MPI неявно передает каждой машине агрументы `rank` и `size`, соответственно, номер машины и размер кластера, при запуске этого файла.

Главными функциями в MPI являются **функции обмена сообщениями**; их много, но они все имеют схожую сигнатуру, например, `MPI_Send(buf, count, datatype, dest, tag, comm)` отправляет данные из буфера `buf`, хранящего `count` объектов типа `datatype` на машину с номером через коммуникатор `comm` (`tag` --- название говорит за себя --- указывать не обязательно), а `MPI_Recv(buf, count, datatype, source, tag, comm, status)`, с аналогичными параметрами, получает данные и сохраняет статус операции по указателю `status`. Еще есть, например, `MPI_Bcast`, через который можно передать данные всем другим машинам.

*В MPI используются свои базовые типы и на их основе можно создавать свои структуры.*

Для MPI, как и для параллельных фреймворков, свойственны дедлоки вследствие взаимного ожидания сообщений. Для профилировки есть отдельные средства, которые собирают трассы с MPI-кластера.
<!--stackedit_data:
eyJoaXN0b3J5IjpbODEzNTA2MDcxXX0=
-->