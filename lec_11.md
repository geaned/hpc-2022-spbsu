# Лекция 11

## OpenMP

**OpenMP (Open Multi-Processing)** --- стандарт интерфейса для многопоточного программирования над общей памятью. Поддерживается всеми современными компиляторами C/C++ и Fortran.

OpenMP демонстрирует редкий пример использования директив препроцессора в параллельном програмировании, к примеру:

```c++
#include <stdio.h>
#include <omp.h>
int main() {
	printf("Hello, World!\n");
	#pragma omp parallel
	{
		int i, n;
		i = omp_get_thread_num();
		n = omp_get_num_threads();
		printf("I'm thread %d of %d\n", i, n);
	}
	return 0;
}
```

*Какие еще нам известны директивы `pragma`? `pragma once` --- контроль подключения исходного файла строго единожды; `pragma pack` задает выравнивание полей структур данных по заданному размеру (вместо того, чтобы выравнивать по границам машинного слова).*

Код, находящийся внутри фигурных скобок, выносится в отдельную функцию, далее создается количество потоков, равное количеству ядер, и генерируется необходимый код, создающий потоки и выполняющий `join()`. Еще пример:

```c++
#include <stdio.h>
#include <omp.h>
int main() {
	int i;

	#pragma omp parallel for
		for (i = 0; i < 1000; i++)
			printf("%d ", i);

	return 0;
}
```

Директива `#pragma omp sections { ... }` вместе с несколькими `#pragma omp section` внутри задает секции, через которые должно пройти по одному потоку (еще есть `#pragma omp single { ... }`, который сразу задает код, который должен исполниться одним потоком).

Можно также подсказать OpenMP, как мы хотим, чтобы разбирались задачи. Например в коде:

```c++
#include <stdio.h>
#include <omp.h>
int main() {
	int i;

	#pragma omp parallel private(i)
	{
		#pragma omp for schedule(static,10) nowait
			for (i = 0; i < 1000; i++) printf("%d ", i);
		#pragma omp for schedule(dynamic,1)
			for (i = 'a'; i <= 'z'; i++) printf("%c ", i);
	}
	return 0;
}	
```

В первом случае OpenMP скажет создавать потоки, которые будут выполнять группы по 10 задач, а во втором случае создастся набор потоков, которые будут выполнять задачи по одной и смотреть, остались ли еще задачи (имеем своеобразную очередь задач). Модификатор переменной `private` скажет, чтобы она была своя для каждого потока, а `shared` говорит, что переменная общая **(но никто не гарантирует потокобезопасность)**. Еще есть модификатор `reduction`, который задает переменную, которая будет локальной для каждого потока, и функцию, которая будет применена к набору из этих переменных по окончании параллельной секции. Например, при выполнении:

```c++
int i, s = 0;

#pragma omp parallel for reduction(+:s)
	for (i = 0; i < 100; i++)
		s += i;
	
printf("Sum: %d\n", s)
```

Локальные суммы `s` просуммируются в общую.

Также есть директивы синхронизации потоков, например, `#pragma omp critical` задает критическую секцию.

Также стандарт OpenMP позволяет произвести миграцию вычислений (и данных, естественно) на поддерживаемое устройство, к примеру, директива `#pragma offload target(mic)` (директива идет перед `#pragma omp ...`) производит выгрузку на MIC (Many Integrated Core), если таковой есть в системе.

## TBB

**TBB (Threading Building Blocks)** --- открытая библиотека от Intel для многопоточного программирования на C++.

У нее есть принципиальные архитектурые особенности, например, в отличие от изобилия `ThreadPool` на Java, в TBB вроде работает через **один пул потоков со своим планировщиком на весь процесс**. Пример:

```c++
int main() {
	int a[N];
	// ...
	tbb::parallel_for (
		tbb::blocked_range<size_t>(0,N,1),	// последний параметр: гранулярность параллелизма
		[&](const tbb::blocked_range<size_t>& r) {
			for (int i = r.begin(); i != r.end(); i++) {
				// ...
			}
		}
	);
	return 0;
```

С помощью параметра гранулярности можно отдельно задать, из диапазонов какого размера будут формироваться отдельные задачи, которые будут отправляться в пул потоков.

Если же у какого-то потока задачи кончатся быстрее, то применяется техника **work stealing**: освободившийся поток возмет часть невыполненых другим потоком задач с конца. Основная причина использования такой техники --- отпимизация переиспользования кэша процессора.

TBB в том числе может сделать **дерево задач**. Они реализованы в TBB, в частности, для решения рекурсивных задач (также для ускорения работы задачи можно отменять при необходимости).

### Аллокаторы

Также в TBB реализованы различные аллокаторы памяти. Один из них --- `cache_aligned_allocator` --- призван бороться с **false sharing** --- проблемой разделения линеек кэша процессора с данными, которые на самом деле разделять не нужно

*Слегка кустарный пример проявления false sharing: пусть мы параллелим цикл, в котором к объектам размера по 32 байт применяется однообразная обработка, по двум потокам, причем первый поток работает с четными элементами, второй --- с нечетными. Из-за того, что размер линейки кэша равен 64 байтам, для каждый пары объектов одна и та же линейка будет копироваться в кэш обоих ядер и она будет помечена, как shared. По этой причине ее **придется постоянно синхронизировать**, тратя лишнее время.*

Еще один аллокатор --- `scalable_allocator` --- работает так: когда поток обращается к нему в первый раз, ему выделяется некоторая область памяти, чуть большая, чем требовалось, тогда при последующих обращениях мы можем **сразу выдать место внутри буфера**, созданного под этого поток (записи с указателями на данные в промежуточном буфере для потоков хранятся в специальной структуре аллокатора). Эта идея сильно ускоряет работу структур данных, которым нужно работать с общей оперативной памятью.

### Parallel pipeline

Пусть у нас есть набор алгоритмов разной вычислительной сложности, которые нужно последовательно применить к некоторым данным. С одной стороны, каждый алгоритм можно выкинуть в отдельный поток, с другой стороны, если какой-то из узлов не успевает, можно его распараллелить и агрегировать выходные данные узла в нужном порядке. Это и есть **пайплайн (pipeline)**. Пример:

```c++
parallel_pipeline(threadCount,
	tbb::make_filter<void, std::string*>(
		tbb:filter:serial,
		[&in](tbb::flow_control& fc) -> std::string* {
			auto line = new std::string();
			getline(in, *line);
			if (!in.eof() && line->length() == 0) {
				fc.stop();
				delete line;
				line = 0;
			}
			return line;
		}
	)
	&
	tbb::make_filter<std::string*, std::string*>(
		tbb::filter::parallel,
		[](std::string* line) {
			tbb::parallel_sort(line->begin(), line->end());
			return line;
		}
	)
	&
	tbb::make_filter<std::string*, void>(
		tbb::filter::serial,
		[&out](std::string* line) {
			out << *line << std::endl;
			delete line;
		}
	)
);
```

Конкурентные структуры данных в TBB расшираются за счет различных примитивов, аллокаторов и т.п., но чуть больше они расширяются с помощью механизма **flow graph**.

Этот механизм позволяет представить свой пайплайн в виде графа, отображающего логику пайплайна. Можно запрограммировать этот граф руками, а можно --- с помощью графической утилиты. Во время исполнения этого графа TBB **сам видит, где происходит просадка производительности**, и расширяет те узлы, которым нужно больше вычислительных ресурсов. Также в графе можно проставить брейкпоинты и, как в большинстве современных IDE, по ним можно провести профилировку.

*Говоря о графических механизмах проектирования, стоит упомянуть **BPEL (Business Process Execution Languge)** --- стандарт проектирования и исполнения бизнес-процессов. BPEL связывает различные API и позволяет с помощью схемы, задающей алгоритм, обработку исключительных ситуаций и т.п., огранизовать бизнес-процесс (который будет исполняться на сервере, умеющем интерпретировать BPEL-схемы).*

## Консенсус

В параллельном программировании, **консенсус** --- это wait-free функция с сигнатурой `T decide(T value)`, которую каждый поток может выполнить один раз. Все потоки должны получить одно и то же нетривиальное значение, то есть оно должно зависеть от `value`. **Консенсусное число** --- максимальное число потоков, для которых существует консенсус `decide()`, реализованный с помощью конкретного примитива синхронизации.

Первый пример: пусть у нас есть атомарный регистр с операциями `read()` и `write(T value)`. Консенсусное число для этого регистра равно **единице**. У такой структуры никак не получится позволить даже двум потокам договориться об общем значении без мьютекса.

Второй пример: RMW-регистр с операцией `getAndSet(T value)`. Для него консенсусное число --- **два**, каждый поток может сравнить результат `getAndSet()` с исходным значением и при равенстве вернуть свое `value`, а при неравенстве вернуть то, что получил из функции.

Третий пример: CAS-операция. Для нее консенсусное число равно **бесконечности** за счет кардинального отличия от RMW-регистра, заключающегося в том, что CAS не пишет в регистр безусловно, в отличие от RMW. Таким образом, мы можем писать `value` в регистр только если значение равно исходному (и возвращать его же), а в проитвном случае возвращать значение из регистра.

Важный вывод: CAS-инструкция в процессоре нужна не просто так, а именно она теоретически позволяет **реализовать wait-free алгоритм для любого количества потоков**.

*В процессорах на архитектуре ARM вместо CAS используется пара операций load-linked и store-conditional, которые работают следующим образом: load-link считывает указатель, а store-conditional, аналогично CAS, проверит значение или же, если с момента load-link никто не обращался к памяти по переданному указателю, **пройдет сразу**.*
